{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c0102-d5e5-4da1-ace1-8bfe28dac1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e58219-ec90-49c7-a24c-d685a2ef7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae7adc-ccf3-4ef0-925b-adca2f426224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def get_ram_usage():\n",
    "    return psutil.virtual_memory().used / (1024 ** 3) \n",
    "\n",
    "ram_usage_gb = get_ram_usage()\n",
    "print(f\"RAM Usage: {ram_usage_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ce368-afbe-4890-83a9-086d577d866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Get memory usage information\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "# Total RAM in bytes\n",
    "total_memory = memory.total\n",
    "\n",
    "# RAM used in bytes\n",
    "used_memory = memory.used\n",
    "\n",
    "# RAM free in bytes\n",
    "free_memory = memory.available\n",
    "\n",
    "# RAM usage percentage\n",
    "memory_percentage = memory.percent\n",
    "\n",
    "print(f\"Total Memory: {total_memory / (1024 ** 3)} GB\")\n",
    "print(f\"Used Memory: {used_memory / (1024 ** 3)} GB\")\n",
    "print(f\"Free Memory: {free_memory / (1024 ** 3)} GB\")\n",
    "print(f\"Memory Usage: {memory_percentage}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4531fd8-4680-4a33-b48f-c9d2241a0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df8f79-81f7-48e2-b7d7-394bfb931c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec848d-2bb3-4052-bcb4-dc57345ce619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff225ecf-f4a2-4f20-8c9a-0ac1ae7ee925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1409c52-006d-4be3-a11c-c63c3bcb7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0177910-89aa-4834-b59b-ac7014806fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "#path to pre-trained weights of the yokov8 model\n",
    "model = YOLO(f'...')\n",
    "#path to the images folde to be detected\n",
    "results=model.predict(source='...', iou=0.7, conf=0.5, save=True ,device=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0254513-c663-46b0-99ed-68090000d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls=[]\n",
    "for result in results :\n",
    "  boxes =result.boxes\n",
    "  classes=boxes.cls\n",
    "  cls.append(classes)\n",
    "\n",
    "for i in range(len(cls)):\n",
    "  cls[i]=cls[i].cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca507a-260a-4ee1-95c6-9dc43a89974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "box=[]\n",
    "\n",
    "for result in results :\n",
    "  boxes =result.boxes\n",
    "  boxes=boxes.xywh\n",
    "  box.append(boxes)\n",
    "\n",
    "\n",
    "for i in range(len(box)):\n",
    "  box[i]=box[i].cpu().numpy()\n",
    "\n",
    "\n",
    "box[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50ed19f-a4f7-4f3c-ad07-6e0682707e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfa786-0b49-4b22-980d-db1988de5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bin_masks=[]\n",
    "for j in range(len(box)):\n",
    "   boxes1 = box[j]\n",
    "   classes1=cls[j]\n",
    "   boxes1_list=boxes1.tolist()\n",
    "   classes1_list=classes1.tolist()\n",
    "   binary_map = np.zeros((32,640,640), dtype=np.uint8)\n",
    "   for box1, class1 in zip(boxes1_list,classes1_list):\n",
    "       x,y,w,h=box1\n",
    "       x1, y1 = round(x), round(y)\n",
    "       x2, y2 = round(x+w), round(y+h)\n",
    "       binary_map[int(class1), y1:y2, x1:x2] = 1\n",
    "\n",
    "   bin_masks.append(binary_map)\n",
    "\n",
    "del results, cls, box\n",
    "bin_masks[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9799e79f-e613-42e4-8229-f1bc56e1431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(bb):\n",
    "    bb=np.nan_to_num(bb, nan=0)\n",
    "    return bb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d29c25a7-e756-4b00-8d76-bd58168d873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_masks=  list(map(remove_nan, bin_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55007e52-60d5-4ab0-9f82-fd77d72a0612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#path to images of the dataset\n",
    "images_folder = '...'\n",
    "#path to labels of the dataset\n",
    "masks_folder = '...'\n",
    "\n",
    "image_files = os.listdir(images_folder)\n",
    "mask_files = os.listdir(masks_folder)\n",
    "\n",
    "\n",
    "image_files.sort()\n",
    "mask_files.sort()\n",
    "\n",
    "image_mask_pairs = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(images_folder, image_file)\n",
    "    mask_file = image_file.replace('.jpg', '.tiff')\n",
    "    mask_path = os.path.join(masks_folder, mask_file)\n",
    "\n",
    "    if os.path.isfile(mask_path):\n",
    "        image_mask_pairs.append((image_path, mask_path))\n",
    "\n",
    "\n",
    "for image_path, mask_path in image_mask_pairs:\n",
    "    print(\"Image:\", image_path)\n",
    "    print(\"Mask:\", mask_path)\n",
    "    print()\n",
    "del image_files, mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55d7b42c-092f-4ab3-861d-d523949b675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "\n",
    "def apply_clahe(image, clip_limit=0.02):\n",
    "    image_float = image.astype(float) / 255.0\n",
    "\n",
    "    image_clahe = exposure.equalize_adapthist(image_float, clip_limit=clip_limit)\n",
    "\n",
    "    image_clahe = (image_clahe * 255).astype(image.dtype)\n",
    "\n",
    "    return image_clahe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee41ed-8677-462e-a34d-4029eded3d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "\n",
    "image_data = []\n",
    "\n",
    "mask_data = []\n",
    "\n",
    "for img_file, mask_file in image_mask_pairs:\n",
    "    img = Image.open(img_file)\n",
    "    mask= tiff.imread(mask_file)\n",
    "    img_data = np.array(img,dtype=np.float16)\n",
    "    img_data=apply_clahe(img_data)\n",
    "    msk_data =np.array(mask,dtype=np.float16)\n",
    "    image_data.append(img_data)\n",
    "    mask_data.append(msk_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d3e6949-10bd-457d-8fff-94adc2b85660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= list(zip(image_data,mask_data,bin_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2119ec25-fc06-422d-a964-4fdb21f4e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def resize_img(input_image,input_mask,input_bb):\n",
    "  input_image = cv2.resize(input_image, (512,512), interpolation=cv2.INTER_NEAREST)\n",
    "  input_bb = np.transpose(input_bb, axes=[1, 2, 0])\n",
    "  input_bb = cv2.resize(input_bb, (512,512), interpolation=cv2.INTER_NEAREST)\n",
    "  input_mask = np.transpose(input_mask, axes=[1, 2, 0])\n",
    "  input_mask = cv2.resize(input_mask, (512,512), interpolation=cv2.INTER_NEAREST)\n",
    "  return input_image,input_mask,input_bb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "638b9b68-c961-401d-a945-1d6db651abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(input_image,input_mask,input_bb):\n",
    "    if np.random.uniform() > 0.5:\n",
    "        input_image = np.fliplr(input_image)\n",
    "        input_mask = np.fliplr(input_mask)\n",
    "        input_bb = np.fliplr(input_bb)\n",
    "        \n",
    "    return input_image,input_mask,input_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e11d102-4e13-491c-931b-a0facc53cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image_data):\n",
    "    image_data = (image_data - np.min(image_data)) / (np.max(image_data) - np.min(image_data))\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b2b84ec-f7e8-4d86-aaff-df0408f0ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(datapoint):\n",
    "    input_image=datapoint[0]\n",
    "    input_mask= datapoint[1]\n",
    "    input_bb =datapoint[2]\n",
    "    input_image, input_mask, input_bb = resize_img(input_image, input_mask, input_bb)\n",
    "    input_image = normalize(input_image)\n",
    "    input_image, input_mask, input_bb = augment(input_image, input_mask, input_bb)\n",
    "\n",
    "\n",
    "    return input_image, input_mask, input_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44cc8f64-678d-4beb-a558-1eb3b943d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset=list(map(load_image_train,dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efbe4d-e05a-4492-b86a-73767943471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define the dataset and categories\n",
    "dataset = new_dataset\n",
    "categories = [24, 72, 15, 32, 37, 30, 33, 140, 7, 35]\n",
    "\n",
    "# Initialize the test and train datasets\n",
    "test_dataset = []\n",
    "train_dataset = []\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "sum = 0\n",
    "# Loop through each category\n",
    "for i, category_count in enumerate(categories):\n",
    "\n",
    "    \n",
    "    # Calculate the number of images for testing (20%)\n",
    "    num_test_images = int(0.2 * category_count)\n",
    "    \n",
    "    # Split the images into test and train\n",
    "    test_images = dataset[sum:sum + num_test_images]\n",
    "    train_images = dataset[sum + num_test_images:sum + category_count]\n",
    "    \n",
    "    # Update the test and train datasets\n",
    "    test_dataset.extend(test_images)\n",
    "    train_dataset.extend(train_images)\n",
    "    \n",
    "    # Print the category and the number of images in the test dataset\n",
    "    print(f'Category {i + 1}: {num_test_images} images in the test dataset')\n",
    "    sum+=category_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0b951be-4b76-4dae-a5eb-b5e2b418bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array(data):\n",
    "    image=data[0]\n",
    "    bb=data[2]\n",
    "    image=image\n",
    "    final_array=np.concatenate((bb,image),axis=2)\n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afe54468-95f0-4381-96c5-7c5757fdfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list= list(map(convert_to_array,train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "192ab186-2fe5-48f2-8ff5-1c0aa29aeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(map(convert_to_array,test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37866a00-32dc-4c98-abf8-edc1f27901a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,mask,bb = zip(*train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d0e16de-e327-4062-846b-539289bf9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test,mask_test,bb_test = zip(*test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09339845-5c18-4468-9651-d4bb6b60c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rate = 0.12\n",
    "from tensorflow.keras.metrics import  Precision, Recall\n",
    "from tensorflow import keras\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (35,))\n",
    "    \n",
    "    # ... (Encoder) ...\n",
    "    inputs0=inputs[ :, :, :, 32:]\n",
    "    inputs1=inputs[ :, :, :, :32]\n",
    "    skip_connections = []  # To store feature maps from each encoder block\n",
    "    # bb_out = []\n",
    "    # Entry block\n",
    "    x = keras.layers.Conv2D(64, 3, strides=1, padding=\"same\")(inputs0)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(64, 3, strides=1, padding=\"same\")(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "\n",
    "    skip_connections.append(x)\n",
    "    \n",
    "    for filters in [128,256,512,1024]: #1024\n",
    "        x = keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "        x = keras.layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "        \n",
    "        x = keras.layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "        skip_connections.append(x)\n",
    "\n",
    "        \n",
    "     \n",
    "        \n",
    "    skip_connections.pop()\n",
    "    for filters in [512,256,128]: #512\n",
    "        x = keras.layers.Conv2DTranspose(filters, 3,strides=2, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "        skip_connection = skip_connections.pop()\n",
    "        x = keras.layers.concatenate([x, skip_connection])  \n",
    "        \n",
    "        x = keras.layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "        \n",
    "        x = keras.layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "\n",
    "    filters=64   \n",
    "    x = keras.layers.Conv2DTranspose(filters, 3,strides=2, padding=\"same\")(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "    skip_connection = skip_connections.pop()\n",
    "    x = keras.layers.concatenate([x, skip_connection])  \n",
    "        \n",
    "    x = keras.layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.SpatialDropout2D(drop_rate)(x)\n",
    "        \n",
    "    x = keras.layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = keras.layers.Conv2D(num_classes, 1, activation=\"softmax\", padding=\"same\")(\n",
    "        x\n",
    "    )\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1462b25e-8987-4a26-9fe4-048f28c6dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dice_loss_with_l2_regularization(target, predicted, epsilon=1e-7, l2_weight=0.1):\n",
    "    intersection = tf.reduce_sum(predicted * target, axis=[1, 2]) \n",
    "    predicted_square = tf.square(predicted)\n",
    "    target_square = tf.square(target)\n",
    "    union = tf.reduce_sum(predicted_square, axis=[1, 2]) + tf.reduce_sum(target_square, axis=[1, 2])\n",
    "    dice = (2 * intersection + epsilon) / (union + epsilon)\n",
    "    mean_dice_loss = tf.reduce_mean(dice)\n",
    "    \n",
    "    l2_norm = tf.reduce_sum(tf.square(predicted - target), axis=[1, 2])\n",
    "    l2_regularization = l2_weight * tf.reduce_mean(l2_norm)\n",
    "    \n",
    "    total_loss = mean_dice_loss + l2_regularization\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f314f2a4-545c-4631-b9a1-b49861b7bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dice_coef(target, predicted, epsilon=1e-7):\n",
    "    predicted = tf.where(predicted<0.51,0.00,1.00)\n",
    "    intersection = tf.reduce_sum(predicted * target, axis=[1, 2]) \n",
    "    predicted_square = tf.square(predicted)\n",
    "    target_square = tf.square(target)\n",
    "    union = tf.reduce_sum(predicted_square, axis=[1, 2]) + tf.reduce_sum(target_square, axis=[1, 2])\n",
    "    dice = (2 * intersection + epsilon) / (union + epsilon)\n",
    "    mean_dice_loss = -tf.reduce_mean(dice)\n",
    "    return -mean_dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the below cells for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12e6f479-c9c3-4f19-8692-57337d039f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(train_list) * 0.25)\n",
    "\n",
    "train_x = [] \n",
    "valid_x = [] \n",
    "train_y = [] \n",
    "valid_y = []\n",
    "\n",
    "# Split the list into two parts\n",
    "split1 = train_list[:split_point]\n",
    "split2 = train_list[split_point:]\n",
    "\n",
    "valid_x.extend(split1)\n",
    "train_x.extend(split2)\n",
    "\n",
    "#split the ground truth\n",
    "split1 = mask[:split_point]\n",
    "split2= mask[split_point:]\n",
    "\n",
    "valid_y.extend(split1)\n",
    "train_y.extend(split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ee8ccde-fed2-45b2-b9bd-f70ca20cca62",
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.device('/CPU:0'):\n",
    "    train_dataset=tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    valid_dataset=tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f8bcd96-21d9-4d3e-90fb-b47c928da2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_tensor(data, label):\n",
    "    label=tf.cast(label, tf.float32)\n",
    "    return data, label\n",
    "with tf.device('/CPU:0'):\n",
    "    train_dataset= train_dataset.map(cast_to_tensor)\n",
    "    valid_dataset= valid_dataset.map(cast_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3466275-404a-43fe-962a-73591dd88bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    " train_dataset1= train_dataset.batch(2)\n",
    " valid_dataset1= valid_dataset.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4fd8734-92ba-43f4-9a75-07241015e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_x, nil_x, test_y, nil_y= train_test_split(test_list, mask_test, test_size=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3885e126-07f1-4306-9ef8-6ec8f5f412f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataSet\n",
    "with tf.device('/CPU:0'):\n",
    "    test_dataset=tf.data.Dataset.from_tensor_slices((test_x, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05773565-bf11-4774-ad58-66c1fc0cb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataSet\n",
    "def cast_to_tensor(data, label):\n",
    "    label=tf.cast(label, tf.float32)\n",
    "    return data, label\n",
    "with tf.device('/CPU:0'):\n",
    "    test_dataset= test_dataset.map(cast_to_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30c2baa9-24db-407d-952f-2d06a0db82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataSet\n",
    "test_dataset1= test_dataset.batch(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8d34923-2311-42e2-a005-7ae8c04e8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import  Precision, Recall\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class ReduceLearningRate(Callback):\n",
    "    def __init__(self, monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1):\n",
    "        super(ReduceLearningRate, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.reduce_lr = self.min_lr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(self.monitor)\n",
    "\n",
    "        if current_loss is None:\n",
    "            raise ValueError(f\"Monitoring metric '{self.monitor}' is not available.\")\n",
    "\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "            self.reduce_lr = self.model.optimizer.learning_rate\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                if self.reduce_lr > self.min_lr:\n",
    "                    self.reduce_lr = self.reduce_lr*self.factor\n",
    "                    self.model.optimizer.learning_rate.assign(self.reduce_lr)\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"\\nEpoch {epoch + 1}: Reducing learning rate to {self.reduce_lr}\")\n",
    "                self.wait = 0\n",
    "\n",
    "lr_callback = ReduceLearningRate(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "model = get_model(img_size=(512, 512), num_classes=32)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.0003),\n",
    "    loss =dice_loss_with_l2_regularization,\n",
    "     metrics = [ Precision(), Recall(),dice_coef]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64f54b-e647-40a5-9f28-1027aea79c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train the model, doing validation at the end of each epoch.\n",
    "history = model.fit(\n",
    "    train_dataset1, \n",
    "    epochs=60, \n",
    "    validation_data=valid_dataset1, \n",
    "    callbacks=[lr_callback]  # Add the lr_callback to the callbacks list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc66422-b237-4672-b3ad-8b2fe7d259e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_dataset1, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a01c2-3a75-4ffd-ae47-a7daaccb6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_dataset1, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "327fdd7c-0c5e-4c74-b532-b8660f3fcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [] \n",
    "valid_x = [] \n",
    "train_y = [] \n",
    "valid_y = []\n",
    "\n",
    "\n",
    "\n",
    "split_point1 = int(len(train_list) * 0.25)\n",
    "split_point2 = int(len(train_list)*0.5)\n",
    "\n",
    "# Split the list into two parts\n",
    "split1= train_list[:split_point1]\n",
    "split2 = train_list[split_point1:split_point2]\n",
    "split3 = train_list[split_point2:]\n",
    "\n",
    "train_x.extend(split1+split3)\n",
    "valid_x.extend(split2)\n",
    "\n",
    "#split the ground truth\n",
    "split1= mask[:split_point1]\n",
    "split2 = mask[split_point1:split_point2]\n",
    "split3 = mask[split_point2:]\n",
    "\n",
    "train_y.extend(split1+split3)\n",
    "valid_y.extend(split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7639db19-985f-4845-bc38-b76a2812d734",
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.device('/CPU:0'):\n",
    "    train_dataset=tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    valid_dataset=tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f675debf-a2de-45af-8456-badb230891fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_tensor(data, label):\n",
    "    label=tf.cast(label, tf.float32)\n",
    "    return data, label\n",
    "with tf.device('/CPU:0'):\n",
    "    train_dataset= train_dataset.map(cast_to_tensor)\n",
    "    valid_dataset= valid_dataset.map(cast_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d7508c1-8921-499b-82be-2fd7f741b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    " train_dataset1= train_dataset.batch(2)\n",
    " valid_dataset1= valid_dataset.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ead7eb2-89bf-4c22-81bf-e39ddf7e5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import  Precision, Recall\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class ReduceLearningRate(Callback):\n",
    "    def __init__(self, monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1):\n",
    "        super(ReduceLearningRate, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.reduce_lr = self.min_lr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(self.monitor)\n",
    "\n",
    "        if current_loss is None:\n",
    "            raise ValueError(f\"Monitoring metric '{self.monitor}' is not available.\")\n",
    "\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "            self.reduce_lr = self.model.optimizer.learning_rate\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                if self.reduce_lr > self.min_lr:\n",
    "                    self.reduce_lr = self.reduce_lr*self.factor\n",
    "                    self.model.optimizer.learning_rate.assign(self.reduce_lr)\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"\\nEpoch {epoch + 1}: Reducing learning rate to {self.reduce_lr}\")\n",
    "                self.wait = 0\n",
    "\n",
    "lr_callback = ReduceLearningRate(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "model = get_model(img_size=(512, 512), num_classes=32)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.0003),\n",
    "    loss =dice_loss_with_l2_regularization,\n",
    "     metrics = [ Precision(), Recall(),dice_coef]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cfba4-3574-487b-9f58-cbd5efc04e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model, doing validation at the end of each epoch.\n",
    "history = model.fit(\n",
    "    train_dataset1, \n",
    "    epochs=60, \n",
    "    validation_data=valid_dataset1, \n",
    "    callbacks=[lr_callback]  # Add the lr_callback to the callbacks list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9121fb-2969-4475-98c8-2d8f76816fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_dataset1, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e6861-e184-4898-b1cc-9adb301a4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_dataset1, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94b86a17-430b-4c2b-aebc-379aa5b24b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [] \n",
    "valid_x = [] \n",
    "train_y = [] \n",
    "valid_y = []\n",
    "\n",
    "split_point1 = int(len(train_list) * 0.5)\n",
    "split_point2 = int(len(train_list)*0.75)\n",
    "\n",
    "# Split the list into two parts\n",
    "split1= train_list[:split_point1]\n",
    "split2 = train_list[split_point1:split_point2]\n",
    "split3 = train_list[split_point2:]\n",
    "\n",
    "train_x.extend(split1+split3)\n",
    "valid_x.extend(split2)\n",
    "\n",
    "#split the ground truth\n",
    "split1= mask[:split_point1]\n",
    "split2 = mask[split_point1:split_point2]\n",
    "split3 = mask[split_point2:]\n",
    "\n",
    "train_y.extend(split1+split3)\n",
    "valid_y.extend(split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a4635-521e-40fd-a432-7d062d4d0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.device('/CPU:0'):\n",
    "    train_dataset=tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    valid_dataset=tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98c75722-9f5e-443b-a679-80d7f95e989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_tensor(data, label):\n",
    "    label=tf.cast(label, tf.float32)\n",
    "    return data, label\n",
    "with tf.device('/CPU:0'):\n",
    "    train_dataset= train_dataset.map(cast_to_tensor)\n",
    "    valid_dataset= valid_dataset.map(cast_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80a1bd99-5c0a-4dd0-9c0e-d6406cdd0730",
   "metadata": {},
   "outputs": [],
   "source": [
    " train_dataset1= train_dataset.batch(2)\n",
    " valid_dataset1= valid_dataset.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6d2e947-c936-4bcb-9104-a48af600009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import  Precision, Recall\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class ReduceLearningRate(Callback):\n",
    "    def __init__(self, monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1):\n",
    "        super(ReduceLearningRate, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.reduce_lr = self.min_lr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(self.monitor)\n",
    "\n",
    "        if current_loss is None:\n",
    "            raise ValueError(f\"Monitoring metric '{self.monitor}' is not available.\")\n",
    "\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "            self.reduce_lr = self.model.optimizer.learning_rate\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                if self.reduce_lr > self.min_lr:\n",
    "                    self.reduce_lr = self.reduce_lr*self.factor\n",
    "                    self.model.optimizer.learning_rate.assign(self.reduce_lr)\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"\\nEpoch {epoch + 1}: Reducing learning rate to {self.reduce_lr}\")\n",
    "                self.wait = 0\n",
    "\n",
    "lr_callback = ReduceLearningRate(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "model = get_model(img_size=(512, 512), num_classes=32)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.0003),\n",
    "    loss =dice_loss_with_l2_regularization,\n",
    "     metrics = [ Precision(), Recall(),dice_coef]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a1ab1-d4d9-48d3-82ae-98a4e45cef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model, doing validation at the end of each epoch.\n",
    "history = model.fit(\n",
    "    train_dataset1, \n",
    "    epochs=60, \n",
    "    validation_data=valid_dataset1, \n",
    "    callbacks=[lr_callback]  # Add the lr_callback to the callbacks list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7f2f9-593e-491f-9703-f4289a640494",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_dataset1, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae0a48-46d8-4653-b77f-b66eef80b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_dataset1, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ad48a55-c59e-43e7-b65a-7e5730bca4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(train_list) * 0.75)\n",
    "\n",
    "train_x = [] \n",
    "valid_x = [] \n",
    "train_y = [] \n",
    "valid_y = []\n",
    "\n",
    "# Split the list into two parts\n",
    "split1 = train_list[:split_point]\n",
    "split2 = train_list[split_point:]\n",
    "\n",
    "valid_x.extend(split2)\n",
    "train_x.extend(split1)\n",
    "\n",
    "#split the ground truth\n",
    "split1 = mask[:split_point]\n",
    "split2= mask[split_point:]\n",
    "\n",
    "valid_y.extend(split2)\n",
    "train_y.extend(split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03cbd36a-cd47-4106-a13d-97e585c89a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    " with tf.device('/CPU:0'):\n",
    "    train_dataset=tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    valid_dataset=tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ed2ddba-4c41-4d4f-b4ec-276b16a6f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_tensor(data, label):\n",
    "    label=tf.cast(label, tf.float32)\n",
    "    return data, label\n",
    "with tf.device('/CPU:0'):\n",
    "    train_dataset= train_dataset.map(cast_to_tensor)\n",
    "    valid_dataset= valid_dataset.map(cast_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9709c93a-1ec2-42de-aafe-27b5e7b69b37",
   "metadata": {},
   "outputs": [],
   "source": [
    " train_dataset1= train_dataset.batch(2)\n",
    " valid_dataset1= valid_dataset.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c63adb03-5147-4d17-96ca-afd5631343ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import  Precision, Recall\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class ReduceLearningRate(Callback):\n",
    "    def __init__(self, monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1):\n",
    "        super(ReduceLearningRate, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.reduce_lr = self.min_lr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(self.monitor)\n",
    "\n",
    "        if current_loss is None:\n",
    "            raise ValueError(f\"Monitoring metric '{self.monitor}' is not available.\")\n",
    "\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "            self.reduce_lr = self.model.optimizer.learning_rate\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                if self.reduce_lr > self.min_lr:\n",
    "                    self.reduce_lr = self.reduce_lr*self.factor\n",
    "                    self.model.optimizer.learning_rate.assign(self.reduce_lr)\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"\\nEpoch {epoch + 1}: Reducing learning rate to {self.reduce_lr}\")\n",
    "                self.wait = 0\n",
    "\n",
    "lr_callback = ReduceLearningRate(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "model = get_model(img_size=(512, 512), num_classes=32)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.0003),\n",
    "    loss =dice_loss_with_l2_regularization,\n",
    "     metrics = [ Precision(), Recall(),dice_coef]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c96fab-a277-4ef6-b6f8-62370f2909a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model, doing validation at the end of each epoch.\n",
    "history = model.fit(\n",
    "    train_dataset1, \n",
    "    epochs=60, \n",
    "    validation_data=valid_dataset1, \n",
    "    callbacks=[lr_callback]  # Add the lr_callback to the callbacks list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c6663-89ae-4c1e-a93a-72ef780c2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(valid_dataset1, verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30f5a4-96dc-4250-973e-7b443b4d6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_dataset1, verbose = 1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
